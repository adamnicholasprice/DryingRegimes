t<-t %>% replace_na(list(nf_start=0))
#If drying event occurs
if(sum(t$nf_start, na.rm=T)!=0){
#Define recession event as the length of time between peak and longest dry event before the next peak.
#Define all drying events
t<-t %>%
#Number drying events
mutate(dry_event_id = cumsum(nf_start)) %>%
#Remove id number when > 0
mutate(dry_event_id = if_else(q>0, 0, dry_event_id))
#Define longest dry event
dry_event <- t %>%
#Count length of indivdiual drying events
filter(dry_event_id>0) %>%
group_by(dry_event_id) %>%
summarise(
n = n(),
date = min(date)) %>%
#filter to max
arrange(-n, date) %>%
filter(row_number()==1) %>%
#isolate just the date
select(dry_event_id) %>% pull()
#filter data frame to dry event
t<-t %>% filter(dry_event_id==dry_event)
#Create output
output<- tibble(
#event_id
event_id = t$event_id[1],
#Define Year
calendar_year = year(t$date[1]),
#Define season
season = if_else(month(t$date[1])<=3, "Winter",
if_else(month(t$date[1])>3 & month(t$date[1])<=6, "Spring",
if_else(month(t$date[1])>6 & month(t$date[1])<=9, "Summer",
"Fall"))),
#Define meterological year
meteorologic_year = if_else(season == 'Winter',
calendar_year -1,
calendar_year),
#define dry date
dry_date_start = as.POSIXlt(t$date, "%Y-%m-%d")$yday[1],
#Define mean dry date
dry_date_mean = mean(as.POSIXlt(t$date, "%Y-%m-%d")$yday, na.rm = T),
#Estiamte dry duration
dry_dur = nrow(t))
}else{
output<-tibble(
event_id = t$event_id[1],
calendar_year = NA,
season = NA,
meteorologic_year = NA,
dry_date_start = NA,
dry_date_mean = NA,
dry_dur = NA
)
}
#Export
output
}
#Run functions~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
metrics<-lapply(
X = seq(1,max(df$event_id, na.rm=T)),
FUN = function(m){full_join(recession_fun(m), dry_fun(m))}
) %>%
bind_rows() %>%
mutate(gage = gage) %>%
drop_na(dry_dur)
#Export metrics
metrics
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 3: Execute and write-----------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Start timer
t0<-Sys.time()
#Create error handling function
execute<-function(a){
tryCatch(metrics_fun(a), error=function(e){
tibble(
event_id = NA,
peak_date = NA,
peak2zero = NA,
drying_rate = NA,
calendar_year = NA,
season = NA,
meteorologic_year = NA,
dry_date_start = NA,
dry_date_mean = NA,
dry_dur = NA,
p_value = NA,
BayesFactor = NA,
gage = tools::file_path_sans_ext(basename(files))[a])}
)
}
# get number of cores
n.cores <- detectCores()
#start cluster
cl <-  makePSOCKcluster(n.cores)
#Export file list to cluster
clusterExport(cl, c('files', 'metrics_fun'), env=.GlobalEnv)
# Use mpapply to exicute function
x<-parLapply(cl,seq(1, length(files)),execute) #length(files)
# Stop the cluster
stopCluster(cl)
#gather output
output<-bind_rows(x)
plot(output$p_value,output$BayesFactor)
# Get list of files
files <- list.files('../data/daily_data_with_ climate_and_PET/csv',pattern = "*.csv",full.names = TRUE)
# Stop the cluster
stopCluster(cl)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 3: Execute and write-----------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Start timer
t0<-Sys.time()
#Create error handling function
execute<-function(a){
tryCatch(metrics_fun(a), error=function(e){
tibble(
event_id = NA,
peak_date = NA,
peak2zero = NA,
drying_rate = NA,
calendar_year = NA,
season = NA,
meteorologic_year = NA,
dry_date_start = NA,
dry_date_mean = NA,
dry_dur = NA,
p_value = NA,
BayesFactor = NA,
gage = tools::file_path_sans_ext(basename(files))[a])}
)
}
# get number of cores
n.cores <- detectCores()
#start cluster
cl <-  makePSOCKcluster(n.cores)
files = files[1:3]
#Export file list to cluster
clusterExport(cl, c('files', 'metrics_fun'), env=.GlobalEnv)
x<-pbapply::pblappy(cl,seq(1, length(files)),execute)
library(pbapply)
x<-pblappy(cl,seq(1, length(files)),execute)
x<-pblapply(cl,seq(1, length(files)),execute)
seq(1, length(files))
x<-pblapply(seq(1, length(files)),execute,cl=cl)
# Get list of files
files <- list.files('../data/daily_data_with_ climate_and_PET/csv',pattern = "*.csv",full.names = TRUE)
files = files[1:15]
# Stop the cluster
stopCluster(cl)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 3: Execute and write-----------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Start timer
t0<-Sys.time()
#Create error handling function
execute<-function(a){
tryCatch(metrics_fun(a), error=function(e){
tibble(
event_id = NA,
peak_date = NA,
peak2zero = NA,
drying_rate = NA,
calendar_year = NA,
season = NA,
meteorologic_year = NA,
dry_date_start = NA,
dry_date_mean = NA,
dry_dur = NA,
p_value = NA,
BayesFactor = NA,
gage = tools::file_path_sans_ext(basename(files))[a])}
)
}
# get number of cores
n.cores <- detectCores()
#start cluster
cl <-  makePSOCKcluster(n.cores)
#Export file list to cluster
clusterExport(cl, c('files', 'metrics_fun'), env=.GlobalEnv)
# Use mpapply to exicute function
# x<-parLapply(cl,seq(1, length(files)),execute) #length(files)
x<-pblapply(seq(1, length(files)),execute,cl=cl)
# Stop the cluster
stopCluster(cl)
#gather output
output<-bind_rows(x)
plot(output$p_value,output$BayesFactor)
# Get list of files
files <- list.files('../data/daily_data_with_ climate_and_PET/csv',pattern = "*.csv",full.names = TRUE)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 1: Setup workspace ------------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Clear memory
remove(list=ls())
#Libraries (for Windows OS)
library(parallel)
library(lubridate)
library(tidyverse)
library(BayesFactor)
# Get list of files
files <- list.files('../data/daily_data_with_ climate_and_PET/csv',pattern = "*.csv",full.names = TRUE)
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 2: Create Function ------------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create peak 2 zero function
metrics_fun <- function(n){
#For testing
#n<-which(str_detect(files, '14034500'))
#Setup workspace~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Download libraries of interest
library(lubridate)
library(tidyverse)
library(BayesFactor)
#Define gage
gage <- as.character(tools::file_path_sans_ext(basename(files)))[n]
#Download data and clean
df <- read_csv(file = files[n],
col_types = 'dDdddddddd') %>%
mutate(date=as_date(Date),
num_date = as.numeric(Date),
q = X_00060_00003) %>%
select(date, num_date, q) %>%
na.omit()
#Identify inidividual drying events~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Create new collumn with flow data 25% quantile for peak id
df<-df %>%
#Round to nearest tenth
mutate(q = round(q, 1)) %>%
#25% quantile thresholds
mutate(q_peak = if_else(q>quantile(q,0.25),  q, 0))
#Define peaks using slope break method
df<-df %>%
#Define forward and backward slope at each point
mutate(
slp_b = (q_peak-lag(q_peak))/(num_date-lag(num_date)),
slp_f = (lead(q_peak)-q_peak)/(lead(num_date)-num_date),
slp_f = (lead(q_peak)-q_peak)/(lead(num_date)-num_date),
) %>%
#now flag those derivative changes
mutate(peak_flag = if_else(slp_b>0.0001 & slp_f<0, 1,0),
peak_flag = if_else(is.na(peak_flag), 0, peak_flag))
#Define initiation of no flow
df<-df %>%
#Define individual storm events
mutate(event_id = cumsum(peak_flag)+1) %>%
#Flag initiation of no flow
mutate(nf_start = if_else(q == 0 & lag(q) != 0, 1, 0))
#Recession metrics~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
recession_fun<-function(m){
#Isolate indivdual recession events
t<-df %>% filter(event_id == m)
#Convert NA to zero
t<-t %>% replace_na(list(nf_start=0))
#Compute drying regime stats if the following conditions exist
if(sum(t$nf_start, na.rm=T)!=0 & #there is a dry period in this event
t$q[1]!=0 &                   #the event dosn't start with q=0
sum(t$q_peak)!=0){            #there is no peak value
#Define recession event as the length of time between peak and longest dry
#    event before the next peak.
#Define all drying event
t<-t %>%
#Number drying events
mutate(dry_event_id = cumsum(nf_start)) %>%
#Remove id number when > 0
mutate(dry_event_id = if_else(q>0, 0, dry_event_id))
#Define dry date as the start of the longest drying event
dry_date <- t %>%
#Count length of indivdiual drying events
filter(dry_event_id>0) %>%
group_by(dry_event_id) %>%
summarise(
n = n(),
date = min(date)) %>%
#filter to max
arrange(-n, date) %>%
filter(row_number()==1) %>%
#isolate just the date
select(date)
#Dry Date
t<-t %>% filter(date<=dry_date$date)
#Define event_id
event_id <- t$event_id[1]
#Define Peak Data
peak_date <- as.POSIXlt(t$date[1], "%Y-%m-%d")$yday[1]
peak_value <- t$q[1]
peak_quantile <- ecdf(df$q)(peak_value)
#Define Peak to zero metric
peak2zero <- nrow(t)
#Create linear model of dQ vs q
t<- t %>% mutate(dQ = lag(q) - q) %>% filter(dQ>=0)
model<-lm(log10(dQ+0.1)~log10(q+0.1), data=t)
# Extract Bayes Factor
BFy = log(t$dQ+0.1)
BFx = log(t$q+0.1)
temp = as.data.frame(cbind(BFx,BFy))
BF <- lmBF(BFy~BFx,data=temp)
#Estimate drying rate [note the error catch for low slopes]
drying_rate <- tryCatch(model$coefficients[2], error = function(e) NA)
p_value <- tryCatch(summary(model)$coefficients[2,4], error = function(e) NA)
BayesFactor <- tryCatch(extractBF(BF)[1][[1]], error = function(e) NA)
# BayesFactor = BFy
#Create output tibble
output<-tibble(event_id, peak_date, peak_value, peak_quantile, peak2zero, drying_rate, p_value,BayesFactor)
}else{
output<-tibble(
event_id = t$event_id[1],
peak_date = NA,
peak_value = NA,
peak_quantile = NA,
peak2zero = NA,
drying_rate = NA,
p_value = NA,
BayesFactor = NA
)
}
#Export
output
}
#Dry metrics~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
dry_fun<-function(m){
#Isolate indivdual recession events
t<-df %>% filter(event_id == m)
#Convert NA to zero
t<-t %>% replace_na(list(nf_start=0))
#If drying event occurs
if(sum(t$nf_start, na.rm=T)!=0){
#Define recession event as the length of time between peak and longest dry event before the next peak.
#Define all drying events
t<-t %>%
#Number drying events
mutate(dry_event_id = cumsum(nf_start)) %>%
#Remove id number when > 0
mutate(dry_event_id = if_else(q>0, 0, dry_event_id))
#Define longest dry event
dry_event <- t %>%
#Count length of indivdiual drying events
filter(dry_event_id>0) %>%
group_by(dry_event_id) %>%
summarise(
n = n(),
date = min(date)) %>%
#filter to max
arrange(-n, date) %>%
filter(row_number()==1) %>%
#isolate just the date
select(dry_event_id) %>% pull()
#filter data frame to dry event
t<-t %>% filter(dry_event_id==dry_event)
#Create output
output<- tibble(
#event_id
event_id = t$event_id[1],
#Define Year
calendar_year = year(t$date[1]),
#Define season
season = if_else(month(t$date[1])<=3, "Winter",
if_else(month(t$date[1])>3 & month(t$date[1])<=6, "Spring",
if_else(month(t$date[1])>6 & month(t$date[1])<=9, "Summer",
"Fall"))),
#Define meterological year
meteorologic_year = if_else(season == 'Winter',
calendar_year -1,
calendar_year),
#define dry date
dry_date_start = as.POSIXlt(t$date, "%Y-%m-%d")$yday[1],
#Define mean dry date
dry_date_mean = mean(as.POSIXlt(t$date, "%Y-%m-%d")$yday, na.rm = T),
#Estiamte dry duration
dry_dur = nrow(t))
}else{
output<-tibble(
event_id = t$event_id[1],
calendar_year = NA,
season = NA,
meteorologic_year = NA,
dry_date_start = NA,
dry_date_mean = NA,
dry_dur = NA
)
}
#Export
output
}
#Run functions~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
metrics<-lapply(
X = seq(1,max(df$event_id, na.rm=T)),
FUN = function(m){full_join(recession_fun(m), dry_fun(m))}
) %>%
bind_rows() %>%
mutate(gage = gage) %>%
drop_na(dry_dur)
#Export metrics
metrics
}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Step 3: Execute and write-----------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Start timer
t0<-Sys.time()
#Create error handling function
execute<-function(a){
tryCatch(metrics_fun(a), error=function(e){
tibble(
event_id = NA,
peak_date = NA,
peak2zero = NA,
drying_rate = NA,
calendar_year = NA,
season = NA,
meteorologic_year = NA,
dry_date_start = NA,
dry_date_mean = NA,
dry_dur = NA,
p_value = NA,
BayesFactor = NA,
gage = tools::file_path_sans_ext(basename(files))[a])}
)
}
# get number of cores
n.cores <- detectCores()
#start cluster
cl <-  makePSOCKcluster(n.cores)
#Export file list to cluster
clusterExport(cl, c('files', 'metrics_fun'), env=.GlobalEnv)
# Use mpapply to exicute function
# x<-parLapply(cl,seq(1, length(files)),execute) #length(files)
x<-pblapply(seq(1, length(files)),execute,cl=cl)
# Stop the cluster
stopCluster(cl)
#gather output
output<-bind_rows(x)
#Capture finishing time
tf<-Sys.time()
tf-t0
#Write output
output<-output %>% select(gage, calendar_year, meteorologic_year, season,
peak_date, peak2zero, drying_rate,
dry_date_start, dry_date_mean, dry_dur,p_value,BayesFactor
)
write_csv(output,paste0('../data/metrics_by_event_testAP.csv'))
#   )
#
# #Create joined export file
# export<-left_join(bulk, n_event) %>%
#   left_join(., dry_day) %>%
#   pivot_longer(-gage)
#
# #Write output
# write_csv(export,paste0('./data/metrics_by_gage.csv'))
#
write_csv(output,paste0('./data/metrics_by_eventAP.csv'))
#   )
#
# #Create joined export file
# export<-left_join(bulk, n_event) %>%
#   left_join(., dry_day) %>%
#   pivot_longer(-gage)
#
# #Write output
# write_csv(export,paste0('./data/metrics_by_gage.csv'))
#
write_csv(output,paste0('../data/metrics_by_eventAP.csv'))
plot(output$p_value,output$BayesFactor)
ggplot(output$p_value,output$BayesFactor) +geom_point()
ggplot(aes(output$p_value,output$BayesFactor)) +geom_point()
ggplot(data=ourput,aes(p_value,BayesFactor)) +geom_point()
ggplot(data=output,aes(p_value,BayesFactor)) +geom_point()
ggplot(data=output,aes(p_value,BayesFactor)) +geom_point()+ylim(c(0,10))
porosity = c(0:1,by=0.01)
porosity = linspace(0:1,by=0.01)
porosity = seq(0,1,by=0.01)
KW=0.62
KG=2.60
FUN= (KW^porosity)*(KG^(1-porosity))
plot(porosity,FUN)
KW=2.60
KG=2.60
FUN = (KW^porosity)*(KG^(1-porosity))
plot(porosity,FUN)
KW=0
KG=2.60
FUN = (KW^porosity)*(KG^(1-porosity))
plot(porosity,FUN)
FUN = (KG^(1-porosity))
plot(porosity,FUN)
KW=0.62
KG=2.60
FUN = (KW^porosity)*(KG^(1-porosity))
FUN2 = (KG^(1-porosity))
plot(porosity,FUN)
ppoints(porosity,FUN2)
points(porosity,FUN2)
points(porosity,FUN2,col="r")
points(porosity,FUN2,col="red")
KW=0.62
KG=2.60
FUN = (KW^porosity)*(KG^(1-porosity))
FUN2 = (KG^(1-porosity))
plot(porosity,FUN)
points(porosity,FUN2,col="red")
plot(porosity,FUN,type='l')
points(porosity,FUN2,col="red",type='l')
points(porosity,FUN2,col="red",type='l',lwd=2)
plot(porosity,FUN,type='l',lwd=2)
points(porosity,FUN2,col="red",type='l',lwd=2)
plot(porosity,FUN,type='l',lwd=3)
points(porosity,FUN2,col="red",type='l',lwd=3)
